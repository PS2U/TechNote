# 5.1 动机

Spark 支持很多种输入输出源。一部分原因是 Spark 本身是基于 Hadoop 生态圈而构建，特别是 Spark 可以通过 Hadoop MapReduce 所使用的 `InputFormat` 和 `OutputFormat` 接口访问数据，而大部分常见的文件格式与存储系统（例如 S3、HDFS、Cassandra、HBase 等）都支持这种接口。

三类常见的数据源：

- **文件格式与文件系统**。对于存储在本地文件系统或分布式文件系统（比如 NFS、HDFS、Amazon S3 等）中的数据，Spark 可以访问很多种不同的文件格式，包括文本文件、JSON、SequenceFile，以及 protocol buffer。

- **Spark SQL中的结构化数据源**。针对JSON和Apache Hive。

- **数据库与键值存储**。连接如Cassandra、HBase、Elasticsearch、JDBC源。

  ​


# 5.2 文件格式 

Spark 支持的一些常见格式：

| 格式名称          | 结构化  | 备注                                       |
| ------------- | ---- | ---------------------------------------- |
| 文本文件          | 否    | 普通的文本文件，每行一条记录                           |
| JSON          | 半结构化 | 常见的基于文本的格式；大多数库都要求每行一条记录                 |
| CSV           | 是    | 基于文本，通常在电子表格中使用                          |
| SequenceFiles | 是    | 用于键值对数据的常见Hadoop文件格式                     |
| Proto buffers | 是    | 快速、解决空间的跨语言格式                            |
| 对象文件          | 是    | 用来将Spark作业的数据存储下来以让共享的代码读取。改变类的时候它会失效，因为它依赖于Java序列化 |



## 5.2.1 文本文件

当我们将一个文本文件读取为 RDD 时，输入的每一行都会成为 RDD 的一个元素。也可以将多个完整的文本文件一次性读取为pair RDD，键是文件名，值是文件内容。

```scala
val input = sc.textFile("file:///home/holden/README.md")
```

如果想读取整个目录，可以使用`wholeTextFiles()`方法，它会返回一个pair RDD。

```scala
val input = sc.wholeTextFiles("file:///home/holden")
val result = input.mapValues(y => 
                            val nums = y.split(" ").map(x => x.toDouble)
                            nums.sum / nums.size.toDouble)
```

保存文本文件，使用`saveAsTextFile()`方法接受一个路径，将RDD的内容都输入到路径对应的文件中。Spark将传入的路径当做目录，会在目录下输出多个文件。我们不能控制数据的哪一部分输出到哪个文件中，不过有些输出格式支持控制。

```python
result.saveAsTextFile(outputFile)
```

## 5.2.2 JSON

