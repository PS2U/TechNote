# 5.1 动机

Spark 支持很多种输入输出源。一部分原因是 Spark 本身是基于 Hadoop 生态圈而构建，特别是 Spark 可以通过 Hadoop MapReduce 所使用的 `InputFormat` 和 `OutputFormat` 接口访问数据，而大部分常见的文件格式与存储系统（例如 S3、HDFS、Cassandra、HBase 等）都支持这种接口。

三类常见的数据源：

- **文件格式与文件系统**。对于存储在本地文件系统或分布式文件系统（比如 NFS、HDFS、Amazon S3 等）中的数据，Spark 可以访问很多种不同的文件格式，包括文本文件、JSON、SequenceFile，以及 protocol buffer。

- **Spark SQL中的结构化数据源**。针对JSON和Apache Hive。

- **数据库与键值存储**。连接如Cassandra、HBase、Elasticsearch、JDBC源。

  ​


# 5.2 文件格式 

Spark 支持的一些常见格式：

| 格式名称          | 结构化  | 备注                                       |
| ------------- | ---- | ---------------------------------------- |
| 文本文件          | 否    | 普通的文本文件，每行一条记录                           |
| JSON          | 半结构化 | 常见的基于文本的格式；大多数库都要求每行一条记录                 |
| CSV           | 是    | 基于文本，通常在电子表格中使用                          |
| SequenceFiles | 是    | 用于键值对数据的常见Hadoop文件格式                     |
| Proto buffers | 是    | 快速、解决空间的跨语言格式                            |
| 对象文件          | 是    | 用来将Spark作业的数据存储下来以让共享的代码读取。改变类的时候它会失效，因为它依赖于Java序列化 |



## 5.2.1 文本文件

当我们将一个文本文件读取为 RDD 时，输入的每一行都会成为 RDD 的一个元素。也可以将多个完整的文本文件一次性读取为pair RDD，键是文件名，值是文件内容。

```scala
val input = sc.textFile("file:///home/holden/README.md")
```

如果想读取整个目录，可以使用`wholeTextFiles()`方法，它会返回一个pair RDD。

```scala
val input = sc.wholeTextFiles("file:///home/holden")
val result = input.mapValues(y => 
                            val nums = y.split(" ").map(x => x.toDouble)
                            nums.sum / nums.size.toDouble)
```

保存文本文件，使用`saveAsTextFile()`方法接受一个路径，将RDD的内容都输入到路径对应的文件中。Spark将传入的路径当做目录，会在目录下输出多个文件。我们不能控制数据的哪一部分输出到哪个文件中，不过有些输出格式支持控制。

```python
result.saveAsTextFile(outputFile)
```

## 5.2.2 JSON

### 1. 读取JSON

将数据作为文本文件读取，然后对 JSON 数据进行解析，这样的方法可以在所有支持的编程语言中使用。这种方法假设文件中的每一行都是一条 JSON 记录。如果你有跨行的 JSON 数据，你就只能读入整个文件，然后对每个文件进行解析。如果在你使用的语言中构建一个 JSON 解析器的开销较大，你可以使用`mapPartitions()`来重构解析器。

Scala 中使用 Jackson解析 JSON 。 

```scala 
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.databind.DeserializationFeature
...
case class Person(name: String, lovesPandas: Boolean) // 必须是顶级类
...
// 将其解析为特定的case class。使用flatMap，通过在遇到问题时返回空列表（None）
// 来处理错误，而在没有问题时返回包含一个元素的列表（Some(_)）
val result = input.flatMap(record => {
  try {
    Some(mapper.readValue(record, classOf[Person]))
  } catch {
    case e: Exception => None
  }})
  ```

### 2. 保存JSON

写出 JSON 文件不需要考虑格式错误的数据。使用之前将字符串RDD转为解析好的JSON数据的库，将结构化数据组成的RDD转换为字符串RDD，然后用Spark的文本文件API写出去。

```scala 
result.filter(p => P.lovesPandas).map(mapper.writeValueAsString(_))
  .saveAsTextFile(outputFile)
```

## 5.2.3 逗号分隔值与制表符分隔值

逗号分隔值（CSV）文件每行都有固定数目的字段，字段间用逗号隔开（在制表符分隔值文件，即 TSV 文件中用制表符隔开）。记录通常是一行一条，也可以跨行。CSV 文件和 TSV 文件有时支持的标准并不一致，主要是在处理换行符、转义字符、非 ASCII 字符、非整数值等方面。CSV 原生并不支持嵌套字段，所以需要手动组合和分解特定的字段。

与 JSON 中的字段不一样的是，这里的每条记录都没有相关联的字段名，只能得到对应的序号。常规做法是使用第一行中每列的值作为字段名。

### 1. 读取CSV

读取 CSV/TSV 数据和读取 JSON 数据相似，都需要先把文件当作普通文本文件来读取数据，再对数据进行处理。在Scala中使用`opencsv`库。

```scala
import Java.io.StringReader
import au.com.bytecode.opencsv.CSVReader
...
val input = sc.textFile(inputFile)
val result = input.map{ line =>
  val reader = new CSVReader(new StringReader(line));
  reader.readNext();
}
```

如果在字段中嵌有换行符，就需要完整读入每个文件，然后解析各段，

```scala
case class Person(name: String, favoriteAnimal: String)

val input = sc.wholeTextFiles(inputFile)
val result = input.flatMap{ case (_, txt) =>
  val reader = new CSVReader(new StringReader(txt));
  reader.readAll().map(x => Person(x(0), x(1)))
}
```

### 2. 保存CSV

和 JSON 数据一样，写出 CSV/TSV 数据相当简单，同样可以通过重用输出编码器来加速。由于在 CSV 中我们不会在每条记录中输出字段名，因此为了使输出保持一致，需要创建一种映射关系。一种简单做法是写一个函数，用于将各字段转为指定顺序的数组。

```scala 
pandaLovers.map(person => List(person.name, person.favoriteAnimal).toArray)
.mapPartitions{people =>
  val stringWriter = new StringWriter();
  val csvWriter = new CSVWriter(stringWriter);
  csvWriter.writeAll(people.toList)
  Iterator(stringWriter.toString)
}.saveAsTextFile(outFile)
```

## 5.2.4 SequenceFiles

SequenceFile 是由没有相对关系结构的键值对文件组成的常用 Hadoop 格式。SequenceFile 文件有同步标记，Spark可以用它来定位到文件中的某个点，然后再与记录的边界对齐。这可以让 Spark 使用多个节点高效地并行读取 SequenceFile 文件。

SequenceFile 是由 Hadoop 的 `Writable` 接口的元素组成。

### 1. 读取 SequenceFile

```scala 
val data = sc.sequenceFile(inFile, classOf[Text], classOf[IntWritable]).
  map{case (x, y) => (x.toString, y.get())}
```

### 2. 保存 SequenceFile

SequenceFile 存储的是键值对，所以要创建一个可以由可以写出到 SequenceFile 的类型构成的 Pair RDD。

我们已经进行了将许多 Scala 的原生类型转为 Hadoop Writable 的隐式转换，所以如果你要写出的是 Scala 的原生类型，可以直接调用 saveSequenceFile(path) 保存你的 PairRDD ，它会帮你写出数据。如果键和值不能自动转为 Writable 类型，或者想使用变长类型（比如 VIntWritable ），就可以对数据进行映射操作，在保存之前进行类型转换。

```scala
val data = sc.parallelize(List(("Panda", 3), ("Kay", 6), ("Snail", 2)))
data.saveAsSequenceFile(outputFile)
```

## 5.2.5 对象文件

对象文件看起来就像是对 SequenceFile 的简单封装，它允许存储只包含值的 RDD。和 SequenceFile 不一样的是，对象文件是使用 Java 序列化写出的。

