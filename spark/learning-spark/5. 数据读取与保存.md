# 5.1 动机

Spark 支持很多种输入输出源。一部分原因是 Spark 本身是基于 Hadoop 生态圈而构建，特别是 Spark 可以通过 Hadoop MapReduce 所使用的 `InputFormat` 和 `OutputFormat` 接口访问数据，而大部分常见的文件格式与存储系统（例如 S3、HDFS、Cassandra、HBase 等）都支持这种接口。

三类常见的数据源：

- **文件格式与文件系统**。对于存储在本地文件系统或分布式文件系统（比如 NFS、HDFS、Amazon S3 等）中的数据，Spark 可以访问很多种不同的文件格式，包括文本文件、JSON、SequenceFile，以及 protocol buffer。

- **Spark SQL中的结构化数据源**。针对JSON和Apache Hive。

- **数据库与键值存储**。连接如Cassandra、HBase、Elasticsearch、JDBC源。

  ​


# 5.2 文件格式 

Spark 支持的一些常见格式：

| 格式名称          | 结构化  | 备注                                       |
| ------------- | ---- | ---------------------------------------- |
| 文本文件          | 否    | 普通的文本文件，每行一条记录                           |
| JSON          | 半结构化 | 常见的基于文本的格式；大多数库都要求每行一条记录                 |
| CSV           | 是    | 基于文本，通常在电子表格中使用                          |
| SequenceFiles | 是    | 用于键值对数据的常见Hadoop文件格式                     |
| Proto buffers | 是    | 快速、解决空间的跨语言格式                            |
| 对象文件          | 是    | 用来将Spark作业的数据存储下来以让共享的代码读取。改变类的时候它会失效，因为它依赖于Java序列化 |



## 5.2.1 文本文件

当我们将一个文本文件读取为 RDD 时，输入的每一行都会成为 RDD 的一个元素。也可以将多个完整的文本文件一次性读取为pair RDD，键是文件名，值是文件内容。

```scala
val input = sc.textFile("file:///home/holden/README.md")
```

如果想读取整个目录，可以使用`wholeTextFiles()`方法，它会返回一个pair RDD。

```scala
val input = sc.wholeTextFiles("file:///home/holden")
val result = input.mapValues(y => 
                            val nums = y.split(" ").map(x => x.toDouble)
                            nums.sum / nums.size.toDouble)
```

保存文本文件，使用`saveAsTextFile()`方法接受一个路径，将RDD的内容都输入到路径对应的文件中。Spark将传入的路径当做目录，会在目录下输出多个文件。我们不能控制数据的哪一部分输出到哪个文件中，不过有些输出格式支持控制。

```python
result.saveAsTextFile(outputFile)
```

## 5.2.2 JSON

### 1. 读取JSON

将数据作为文本文件读取，然后对 JSON 数据进行解析，这样的方法可以在所有支持的编程语言中使用。这种方法假设文件中的每一行都是一条 JSON 记录。如果你有跨行的 JSON 数据，你就只能读入整个文件，然后对每个文件进行解析。如果在你使用的语言中构建一个 JSON 解析器的开销较大，你可以使用`mapPartitions()`来重构解析器。

Scala 中使用 Jackson解析 JSON 。 

```scala 
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.databind.DeserializationFeature
...
case class Person(name: String, lovesPandas: Boolean) // 必须是顶级类
...
// 将其解析为特定的case class。使用flatMap，通过在遇到问题时返回空列表（None）
// 来处理错误，而在没有问题时返回包含一个元素的列表（Some(_)）
val result = input.flatMap(record => {
  try {
    Some(mapper.readValue(record, classOf[Person]))
  } catch {
    case e: Exception => None
  }})
  ```

### 2. 保存JSON

写出 JSON 文件不需要考虑格式错误的数据。使用之前将字符串RDD转为解析好的JSON数据的库，将结构化数据组成的RDD转换为字符串RDD，然后用Spark的文本文件API写出去。

```scala 
result.filter(p => P.lovesPandas).map(mapper.writeValueAsString(_))
  .saveAsTextFile(outputFile)
```

## 5.2.3 逗号分隔值与制表符分隔值

逗号分隔值（CSV）文件每行都有固定数目的字段，字段间用逗号隔开（在制表符分隔值文件，即 TSV 文件中用制表符隔开）。记录通常是一行一条，也可以跨行。CSV 文件和 TSV 文件有时支持的标准并不一致，主要是在处理换行符、转义字符、非 ASCII 字符、非整数值等方面。CSV 原生并不支持嵌套字段，所以需要手动组合和分解特定的字段。

与 JSON 中的字段不一样的是，这里的每条记录都没有相关联的字段名，只能得到对应的序号。常规做法是使用第一行中每列的值作为字段名。

### 1. 读取CSV

读取 CSV/TSV 数据和读取 JSON 数据相似，都需要先把文件当作普通文本文件来读取数据，再对数据进行处理。在Scala中使用`opencsv`库。

```scala
import Java.io.StringReader
import au.com.bytecode.opencsv.CSVReader
...
val input = sc.textFile(inputFile)
val result = input.map{ line =>
  val reader = new CSVReader(new StringReader(line));
  reader.readNext();
}
```

如果在字段中嵌有换行符，就需要完整读入每个文件，然后解析各段，

```scala
case class Person(name: String, favoriteAnimal: String)

val input = sc.wholeTextFiles(inputFile)
val result = input.flatMap{ case (_, txt) =>
  val reader = new CSVReader(new StringReader(txt));
  reader.readAll().map(x => Person(x(0), x(1)))
}
```

### 2. 保存CSV

和 JSON 数据一样，写出 CSV/TSV 数据相当简单，同样可以通过重用输出编码器来加速。由于在 CSV 中我们不会在每条记录中输出字段名，因此为了使输出保持一致，需要创建一种映射关系。一种简单做法是写一个函数，用于将各字段转为指定顺序的数组。

```scala 
pandaLovers.map(person => List(person.name, person.favoriteAnimal).toArray)
.mapPartitions{people =>
  val stringWriter = new StringWriter();
  val csvWriter = new CSVWriter(stringWriter);
  csvWriter.writeAll(people.toList)
  Iterator(stringWriter.toString)
}.saveAsTextFile(outFile)
```

## 5.2.4 SequenceFiles

SequenceFile 是由没有相对关系结构的键值对文件组成的常用 Hadoop 格式。SequenceFile 文件有同步标记，Spark可以用它来定位到文件中的某个点，然后再与记录的边界对齐。这可以让 Spark 使用多个节点高效地并行读取 SequenceFile 文件。

SequenceFile 是由 Hadoop 的 `Writable` 接口的元素组成。

### 1. 读取 SequenceFile

```scala 
val data = sc.sequenceFile(inFile, classOf[Text], classOf[IntWritable]).
  map{case (x, y) => (x.toString, y.get())}
```

### 2. 保存 SequenceFile

SequenceFile 存储的是键值对，所以要创建一个可以由可以写出到 SequenceFile 的类型构成的 Pair RDD。

我们已经进行了将许多 Scala 的原生类型转为 Hadoop Writable 的隐式转换，所以如果你要写出的是 Scala 的原生类型，可以直接调用 saveSequenceFile(path) 保存你的 PairRDD ，它会帮你写出数据。如果键和值不能自动转为 Writable 类型，或者想使用变长类型（比如 VIntWritable ），就可以对数据进行映射操作，在保存之前进行类型转换。

```scala
val data = sc.parallelize(List(("Panda", 3), ("Kay", 6), ("Snail", 2)))
data.saveAsSequenceFile(outputFile)
```

## 5.2.5 对象文件

对象文件看起来就像是对 SequenceFile 的简单封装，它允许存储只包含值的 RDD。和 SequenceFile 不一样的是，对象文件是使用 Java 序列化写出的。

对对象文件使用 Java 序列化有几个要注意的地方。首先，和普通的 SequenceFile 不同，对于同样的对象，对象文件的输出和 Hadoop 的输出不一样。其次，与其他文件格式不同的是，对象文件通常用于 Spark 作业间的通信。最后，Java 序列化有可能相当慢。

要保存对象文件，只需在 RDD 上调用 `saveAsObjectFile` 就行了。读回对象文件也相当简单：用 SparkContext 中的 `objectFile()` 函数接收一个路径，返回对应的 RDD。

## 5.2.6 Hadoop 输入输出格式

Spark 支持新旧两套 Hadoop 文件 API。

### 1. 读取其他Hadoop输入格式

`newAPIHadoopFile` 接收一个路径以及三个类。第一个类是“格式”类，代表输入格式。相似的函数 hadoopFile() 则用于使用旧的 API 实现的 Hadoop 输入格式。第二个类是键的类，最后一个类是值的类。如果需要设定额外的 Hadoop 配置属性，也可以传入一个 `conf` 对象。

`KeyValueTextInputFormat` 是最简单的 Hadoop 输入格式之一，可以用于从文本文件中读取键值对数据。每一行都会被独立处理，键和值之间用制表符隔开。这个格式存在于 Hadoop 中，所以无需向工程中添加额外的依赖就能使用它。

```scala 
val input = sc.hadoopFile[Text, Text, KeyValueTextInputFormat](inputFile).map{
  case (x, y) => (x.toString, y.toString)
```

Elephant Bird 读取 LZO 算法压缩的 JSON 文件:
```scala
val input = sc.newAPIHadoopFile(inputFile, classOf[LzoJsonInputFormat],
  classOf[LongWritable], classOf[MapWritable], conf)
// "输入"中的每个MapWritable代表一个JSON对象
```

### 2. 保存 Hadoop 输出格式

`saveAsNewHadoopFile`

### 3. 非文件系统数据源

除了 `hadoopFile()` 和 `saveAsHadoopFile()` 这一大类函数，还可以使用 `hadoopDataset/saveAsHadoopDataSet` 和 `newAPIHadoopDataset/saveAsNewAPIHadoopDataset` 来访问 Hadoop 所支持的非文件系统的存储格式。例如，许多像 HBase 和 MongoDB 这样的键值对存储都提供了用来直接读取 Hadoop 输入格式的接口。我们可以在 Spark 中很方便地使用这些格式。

### 4. protocol buffer

PB定义：

```
message Venue {
  required int32 id = 1;
  required string name = 2;
  required VenueType type = 3;
  optional string address = 4;

  enum VenueType {
    COFFEESHOP = 0;
    WORKPLACE = 1;
    CLUB = 2;
    OMNOMNOM = 3;
    OTHER = 4;
  }
}

message VenueResponse {
  repeated Venue results = 1;
}
```

Scala 中使用Elephant Bird写出 protocol buffer：

```scala
val job = new Job()
val conf = job.getConfiguration
LzoProtobufBlockOutputFormat.setClassConf(classOf[Places.Venue], conf);
val dnaLounge = Places.Venue.newBuilder()
dnaLounge.setId(1);
dnaLounge.setName("DNA Lounge")
dnaLounge.setType(Places.Venue.VenueType.CLUB)
val data = sc.parallelize(List(dnaLounge.build()))
val outputData = data.map{ pb =>
  val protoWritable = ProtobufWritable.newInstance(classOf[Places.Venue]);
  protoWritable.set(pb)
  (null, protoWritable)
}
outputData.saveAsNewAPIHadoopFile(outputFile, classOf[Text],
  classOf[ProtobufWritable[Places.Venue]],
  classOf[LzoProtobufBlockOutputFormat[ProtobufWritable[Places.Venue]]], conf)
```