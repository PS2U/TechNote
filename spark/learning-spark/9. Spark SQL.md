![](img/chap9/img0.png)

结构化数据是指任何有结构信息的数据。所谓结构信息，就是每条记录共用的已知的字段集合。当数据符合这样的条件时，Spark SQL 就会使得针对这些数据的读取和查询变得更加简单高效。具体来说，Spark SQL 提供了以下三大功能:

1. Spark SQL可以从各种结构化数据源（JSON、Hive、Parquet等）读取数据。
2. Spark SQL 不仅支持在 Spark 程序内使用 SQL 语句进行数据查询，也支持从类似商业智能软件 Tableau 这样的外部工具中通过标准数据库连接器（JDBC/ODBC）连接 Spark SQL 进行查询。
3. 当在 Spark 程序内使用 Spark SQL 时，Spark SQL 支持 SQL 与常规的 Python/Java/Scala 代码高度整合，包括连接 RDD 与 SQL 表、公开的自定义 SQL 函数接口等。

Spark SQL 提供了一种特殊的 RDD，叫作 SchemaRDD（新版本成为DataFrame）。SchemaRDD 是存放 Row 对象的 RDD，每个 Row 对象代表一行记录。SchemaRDD 还包含记录的结构信息（即数据字段）。SchemaRDD 可以利用结构信息更加高效地存储数据。此外，SchemaRDD 还支持 RDD 上所没有的一些新操作，比如运行 SQL 查询。SchemaRDD 可以从外部数据源创建，也可以从查询结果或普通 RDD 中创建。


# 9.1 连接Spark SQL

要在应用中引入 Spark SQL 需要添加一些额外的依赖。

带有 Hive 支持的 Spark SQL的 sbt：

```properties
libraryDependencies ++= Seq("org.apache.spark" %% "spark-hive_2.1.0" % "1.2.0")
```

如果你不能引入 Hive 依赖，那就应该使用工件 `spark-sql_2.10` 来代替 `spark-hive_2.10` 。

使用Spark SQL编程时，根据是否使用 Hive 支持，有两个不同的入口：

1. `HiveContext` ，它可以提供 HiveQL 以及其他依赖于 Hive 的功能的支持。使用`HiveContext`不需要实现部署好Hive。
2. 更为基础的 SQLContext 则支持 Spark SQL 功能的一个子集，子集中去掉了需要依赖于 Hive 的功能。

最后，若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 `hive-site.xml` 复制到 Spark 的配置文件目录中（`$SPARK_HOME/conf`）。即使没有部署好 Hive，Spark SQL 也可以运行。



# 9.2 在应用中使用Spark SQL

基于已有的 `SparkContext` 创建出一个 `HiveContext`（如果使用的是去除了 Hive 支持的 Spark 版本，则创建出 `SQLContext`）。这个上下文环境提供了对 Spark SQL 的数据进行查询和交互的额外函数。使用 `HiveContext` 可以创建出表示结构化数据的 SchemaRDD，并且使用 SQL 或是类似 `map()` 的普通 RDD 操作来操作这些 SchemaRDD。

## 9.2.1 初始化Spark SQL

```scala
// 导入Spark SQL
import org.apache.spark.sql.hive.HiveContext
// 如果不能使用hive依赖的话
import org.apache.spark.sql.SQLContext
  
val sc = new SparkContext(...)
// 隐式转换
val hiveCtx = new HiveContext(sc)
```

## 9.2.2 基本查询示例

要在一张数据表上进行查询，需要调用 `HiveContext`或` SQLContext` 中的 `sql()` 方法。要做的第一件事就是告诉 Spark SQL 要查询的数据是什么。因此，需要先从 JSON 文件中读取一些推特数据，把这些数据注册为一张临时表并赋予该表一个名字，然后就可以用 SQL 来查询它了。

```scala
val input = hiveCtx.jsonFile(inputFile)
// 注册输入的SchemaRDD
input.registerTempTable("tweets")
// 依据retweetCount（转发计数）选出推文
val topTweets = hiveCtx.sql("SELECT text, retweetCount FROM
  tweets ORDER BY retweetCount LIMIT 10")
```

## 9.2.3 SchemaRDD

读取数据和执行查询都会返回 SchemaRDD。SchemaRDD 和传统数据库中的表的概念类似。从内部机理来看，SchemaRDD 是一个由 Row 对象组成的 RDD，附带包含每列数据类型的结构信息。Row 对象只是对基本数据类型（如整型和字符串型等）的数组的封装。

SchemaRDD 仍然是 RDD，所以你可以对其应用已有的 RDD 转化操作，比如 `map()` 和 `filter()` 。然而，SchemaRDD 也提供了一些额外的功能支持。最重要的是，你可以把任意 SchemaRDD 注册为临时表（`registerTempTable()`），这样就可以使用 `HiveContext.sql` 或 `SQLContext.sql` 来对它进行查询了。

SchemaRDD 可以存储一些基本数据类型，也可以存储由这些类型组成的结构体和数组。

SchemaRDD 使用 [HiveQL 语法](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL)定义的类型。

Row 对象表示 SchemaRDD 中的记录，其本质就是一个定长的字段数组。在 Scala 中，Row 对象有一系列 getter 方法，可以通过下标获取每个字段的值。

```scala
val topTweetText = topTweets.map(row => row.getString(0))
```

## 9.2.4 缓存

Spark SQL 的缓存机制与 Spark 中的稍有不同。由于我们知道每个列的类型信息，所以 Spark 可以更加高效地存储数据。为了确保使用更节约内存的表示方式进行缓存而不是储存整个对象，应当使用专门的 `hiveCtx.cacheTable("tableName")` 方法。当缓存数据表时，Spark SQL 使用一种列式存储格式在内存中表示数据。这些缓存下来的表只会在驱动器程序的生命周期里保留在内存中，所以如果驱动器进程退出，就需要重新缓存数据。



# 9.3 读取和存储数据

