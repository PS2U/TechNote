# 8.1 使用`SparkConf`配置Spark

创建出一个 `SparkContext` 时，就需要创建出一个 `SparkConf` 实例。

```scala 
// 创建一个conf对象
val conf = new SparkConf()
conf.set("spark.app.name", "My Spark App")
conf.set("spark.master", "local[4]")
conf.set("spark.ui.port", "36000") // 重载默认端口配置

// 使用这个配置对象创建一个SparkContext
val sc = new SparkContext(conf)
```

`SparkConf` 实例包含用户要重载的配置选项的键值对。Spark 中的每个配置选项都是基于字符串形式的键值对。

除了在代码中设置，Spark 允许通过 `spark-submit` 工具动态设置配置项。当应用被 `spark-submit` 脚本启动时，脚本会把这些配置项设置到运行环境中。当一个新的 `SparkConf` 被创建出来时，这些环境变量会被检测出来并且自动配好。这样，在使用 `spark-submit` 时，用户应用中只要创建一个“空”的 `SparkConf` ，并直接传给 `SparkContext` 的构造方法就行了。

```shell
$ bin/spark-submit \
  --class com.example.MyApp \
  --master local[4] \
  --name "My Spark App" \
  --conf spark.ui.port=36000 \
  myApp.jar
```

`spark-submit` 也支持从文件中读取配置项的值。这对于设置一些与环境相关的配置项比较有用，方便不同用户共享这些配置（比如默认的 Spark 主节点）。默认情况下，`spark-submit` 脚本会在 Spark 安装目录中找到 `conf/spark-defaults.conf` 文件，尝试读取该文件中以空格隔开的键值对数据。你也可以通过 `spark-submit` 的 `--properties-File` 标记，自定义该文件的路径。

```
$ bin/spark-submit \
  --class com.example.MyApp \
  --properties-file my-config.conf \
  myApp.jar

## Contents of my-config.conf ##
spark.master    local[4]
spark.app.name  "My Spark App"
spark.ui.port   36000
```

Spark 有特定的优先级顺序来选择实际配置。优先级最高的是在用户代码中显式调用 `set()` 方法设置的选项。其次是通过 `spark-submit` 传递的参数，再次是写在配置文件中的值，最后是系统的默认值。



# 8.2 Spark执行的组成部分：作业、任务和步骤

Spark 执行时有下面所列的这些流程:

1. 用户代码定义RDD的有向无环图

   RDD 上的操作会创建出新的 RDD，并引用它们的父节点，这样就创建出了一个图。

2. 行动操作把有向无环图强制转译为执行计划

   当你调用 RDD 的一个行动操作时，这个 RDD 就必须被计算出来。这也要求计算出该 RDD 的父节点。Spark 调度器提交一个作业 来计算所有必要的 RDD。这个作业会包含一个或多个步骤 ，每个步骤其实也就是一波并行执行的计算任务 。一个步骤对应有向无环图中的一个或多个 RDD，一个步骤对应多个 RDD 是因为发生了流水线执行 。

3. 任务于集群中调度并执行

   步骤是按顺序处理的，任务则独立地启动来计算出RDD的一部分。一旦作业的最后一个步骤结束，一个行动操作也就执行完毕了。



# 8.3 查找信息

