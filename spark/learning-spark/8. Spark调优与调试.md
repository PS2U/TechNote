# 8.1 使用`SparkConf`配置Spark

创建出一个 `SparkContext` 时，就需要创建出一个 `SparkConf` 实例。

```scala 
// 创建一个conf对象
val conf = new SparkConf()
conf.set("spark.app.name", "My Spark App")
conf.set("spark.master", "local[4]")
conf.set("spark.ui.port", "36000") // 重载默认端口配置

// 使用这个配置对象创建一个SparkContext
val sc = new SparkContext(conf)
```

SparkConf 实例包含用户要重载的配置选项的键值对。Spark 中的每个配置选项都是基于字符串形式的键值对。

除了在代码中设置，Spark 允许通过 `spark-submit` 工具动态设置配置项。当应用被 `spark-submit` 脚本启动时，脚本会把这些配置项设置到运行环境中。当一个新的 SparkConf 被创建出来时，这些环境变量会被检测出来并且自动配好。这样，在使用 `spark-submit` 时，用户应用中只要创建一个“空”的 `SparkConf` ，并直接传给 `SparkContext` 的构造方法就行了。

```shell
$ bin/spark-submit \
  --class com.example.MyApp \
  --master local[4] \
  --name "My Spark App" \
  --conf spark.ui.port=36000 \
  myApp.jar
```

`spark-submit` 也支持从文件中读取配置项的值。这对于设置一些与环境相关的配置项比较有用，方便不同用户共享这些配置（比如默认的 Spark 主节点）。默认情况下，`spark-submit` 脚本会在 Spark 安装目录中找到 `conf/spark-defaults.conf` 文件，尝试读取该文件中以空格隔开的键值对数据。你也可以通过 `spark-submit` 的 `--properties-File` 标记，自定义该文件的路径。

```
$ bin/spark-submit \
  --class com.example.MyApp \
  --properties-file my-config.conf \
  myApp.jar

## Contents of my-config.conf ##
spark.master    local[4]
spark.app.name  "My Spark App"
spark.ui.port   36000
```

Spark 有特定的优先级顺序来选择实际配置。优先级最高的是在用户代码中显式调用 `set()` 方法设置的选项。其次是通过 `spark-submit` 传递的参数，再次是写在配置文件中的值，最后是系统的默认值。

