# 8.1 使用`SparkConf`配置Spark

创建出一个 `SparkContext` 时，就需要创建出一个 `SparkConf` 实例。

```scala 
// 创建一个conf对象
val conf = new SparkConf()
conf.set("spark.app.name", "My Spark App")
conf.set("spark.master", "local[4]")
conf.set("spark.ui.port", "36000") // 重载默认端口配置

// 使用这个配置对象创建一个SparkContext
val sc = new SparkContext(conf)
```

`SparkConf` 实例包含用户要重载的配置选项的键值对。Spark 中的每个配置选项都是基于字符串形式的键值对。

除了在代码中设置，Spark 允许通过 `spark-submit` 工具动态设置配置项。当应用被 `spark-submit` 脚本启动时，脚本会把这些配置项设置到运行环境中。当一个新的 `SparkConf` 被创建出来时，这些环境变量会被检测出来并且自动配好。这样，在使用 `spark-submit` 时，用户应用中只要创建一个“空”的 `SparkConf` ，并直接传给 `SparkContext` 的构造方法就行了。

```shell
$ bin/spark-submit \
  --class com.example.MyApp \
  --master local[4] \
  --name "My Spark App" \
  --conf spark.ui.port=36000 \
  myApp.jar
```

`spark-submit` 也支持从文件中读取配置项的值。这对于设置一些与环境相关的配置项比较有用，方便不同用户共享这些配置（比如默认的 Spark 主节点）。默认情况下，`spark-submit` 脚本会在 Spark 安装目录中找到 `conf/spark-defaults.conf` 文件，尝试读取该文件中以空格隔开的键值对数据。你也可以通过 `spark-submit` 的 `--properties-File` 标记，自定义该文件的路径。

```
$ bin/spark-submit \
  --class com.example.MyApp \
  --properties-file my-config.conf \
  myApp.jar

## Contents of my-config.conf ##
spark.master    local[4]
spark.app.name  "My Spark App"
spark.ui.port   36000
```

Spark 有特定的优先级顺序来选择实际配置。优先级最高的是在用户代码中显式调用 `set()` 方法设置的选项。其次是通过 `spark-submit` 传递的参数，再次是写在配置文件中的值，最后是系统的默认值。



# 8.2 Spark执行的组成部分：作业、任务和步骤

Spark 执行时有下面所列的这些流程:

1. 用户代码定义RDD的有向无环图

   RDD 上的操作会创建出新的 RDD，并引用它们的父节点，这样就创建出了一个图。

2. 行动操作把有向无环图强制转译为执行计划

   当你调用 RDD 的一个行动操作时，这个 RDD 就必须被计算出来。这也要求计算出该 RDD 的父节点。Spark 调度器提交一个作业 来计算所有必要的 RDD。这个作业会包含一个或多个步骤 ，每个步骤其实也就是一波并行执行的计算任务 。一个步骤对应有向无环图中的一个或多个 RDD，一个步骤对应多个 RDD 是因为发生了流水线执行 。

3. 任务于集群中调度并执行

   步骤是按顺序处理的，任务则独立地启动来计算出RDD的一部分。一旦作业的最后一个步骤结束，一个行动操作也就执行完毕了。




# 8.3 查找信息

Spark 在应用执行时记录详细的进度信息和性能指标可以在两个地方找到：

1. Spark的网页界面
2. 驱动器和执行器进程的日志文件

## 8.3.1 Spark网页用户界面

默认情况下，它在驱动器程序所在机器的 4040 端口上。对于 YARN 集群模式来说，应用的驱动器程序会运行在集群内部，你应该通过 YARN 的资源管理器来访问用户界面。YARN 的资源管理器会把请求直接转发给驱动器程序。

用户界面由4个不同页面组成：

1. 作业界面。步骤与任务的进度和指标，以及更多内容
2. 存储界面。已缓存的RDD的信息
3. 执行器界面。应用中的执行器进程列表
4. 环境界面。调试Spark配置项

## 8.3.2 驱动器进程和执行器进程的日志

Spark 日志文件的具体位置取决于部署模式：

- 在 Spark 独立模式下，所有日志会在独立模式*主节点*的网页用户界面中直接显示。这些日志默认存储于各个工作节点的 Spark 目录下的 `work/` 目录。
- 在 Mesos 模式下，日志存储在 Mesos *从节点*的` work/` 目录中，可以通过 Mesos 主节点用户界面访问。
- 在 YARN 模式下，最简单的收集日志的方法是使用 YARN 的日志收集工具（运行 `yarn logs -applicationId <app ID>` ）来生成一个包含应用日志的报告。

Spark 的日志系统是基于广泛使用的 Java 日志库 log4j 实现的，使用 log4j 的配置方式进行配置。log4j 配置的示例文件已经打包在 Spark 中，具体位置是 `conf/log4j.properties.template`。要想自定义 Spark 的日志，首先需要把这个示例文件复制为 `log4j.properties`，然后就可以修改日志行为了。

# 8.4 关键性能考量





