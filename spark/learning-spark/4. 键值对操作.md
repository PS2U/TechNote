键值对 RDD 是 Spark 中许多操作所需要的常见数据类型。键值对 RDD 通常用来进行聚合计算。我们一般要先通过一些初始 ETL（抽取、转化、装载）操作来将数据转化为键值对形式。键值对 RDD 提供了一些新的操作接口。

# 4.1 动机

Pair RDD 是很多程序的构成要素，因为它们提供了并行操作各个键或跨节点重新进行数据分组的操作接口。例如，pair RDD 提供 reduceByKey() 方法，可以分别归约每个键对应的数据，还有 join() 方法，可以把两个 RDD 中键相同的元素组合到一起，合并为一个 RDD。

# 4.2 创建 Pair RDD

很多存储键值对的数据格式会在读取时直接返回由其键值对数据组成的 pair RDD。此外，当需要把一个普通的 RDD 转为 pair RDD 时，可以调用 `map()` 函数来实现，传递的函数需要返回键值对。

使用第一个单词作为键创建出一个 pair RDD

```scala
val pairs = lines.map(x => (x.split(" ")(0), x))
```

当用 Scala 从一个内存中的数据集创建 pair RDD 时，只需要对这个由二元组组成的集合调用 `SparkContext.parallelize()` 方法

# 4.3 Pair RDD 的转化操作

Pair RDD 可以使用所有标准 RDD 上的可用的转化操作。传递函数的规则也适用于pair RDD。由于pair RDD是二元组，所以要传递的函数应该是操作二元组，而不是独立的元素。

以键值对集合{(1, 2), (3, 4), (3, 6)}为例：

| 函数名                                      | 目的                                       | 示例                                 | 结果                                       |
| ---------------------------------------- | ---------------------------------------- | ---------------------------------- | ---------------------------------------- |
| `reduceByKey(func)`                      | 合并具有相同键的值                                | `rdd.reduceByKey((x, y) => x + y)` | {(1, 2), (3, 10)}                        |
| `groupByKey()`                           | 对具有相同键的值进行分组                             | `rdd.groupByKey()`                 | {(1, [2], (3, [4, 6]))}                  |
| `combineByKey(createCobiner, mergeValue, mergeCombiners, partitioner)` | 使用不同的返回类型合并具有相同键的值                       | 见下文                                |                                          |
| `mapValues(func)`                        | 对pair RDD的每个值应用一个函数而不改变键                 | `rdd.mapValues(x = > x + 1)`       | {(1, 3), (3, 5), (3, 7)}                 |
| `flatMapValues(func)`                    | 对pair RDD的每个应用一个返回迭代器的函数，然后对返回的每个元素都生成一个对应原键的键值对记录 | `rdd.flatMapValues(x = (x to 5))`  | {(1, 2), (1, 3), (1, 4), (1, 5), (3, 4), (3, 5)} |
| `keys()`                                 | 返回一个仅包含键的RDD                             | `rdd.keys`                         | {1, 3, 3}                                |
| `values()`                               | 返回一个仅包含值的RDD                             | `rdd.values`                       | {2, 4, 6}                                |
| `sortByKey()`                            | 返回一个根据键排序的RDD                            | `rdd.sortByKey()`                  | {(1, 2), (3, 4), (3, 6)}                 |

两个pair RDD的转换操作，rdd = {(1, 2), (3, 4), (3, 6)} other = {(3, 9)}

| 函数名              | 目的                              | 示例                          | 结果                                       |
| ---------------- | ------------------------------- | --------------------------- | ---------------------------------------- |
| `subtractByKey`  | 删掉RDD中键与other中的键相同的元素           | `rdd.subtractByKey(other)`  | {(1, 2)}                                 |
| `join`           | 对两个RDD内连接                       | `rdd.join(other)`           | {(3, (4, 9)), (3, (6, 9))}               |
| `rightOuterJoin` | 对两个RDD进行连接，确保第一个RDD的键必须存在（右外连接） | `rdd.rightOuterJoin(other)` | {(3, (some(4), 9)), (3, (some(6), 9))}   |
| `leftOuterJoin`  | 对两个RDD进行连接，确保第二个RDD的键必须存在（左外连接） | `rdd.leftOuterJoin(other)`  | {(1,(2,None)), (3, (4,Some(9))), (3, (6,Some(9)))} |
| `cogroup`        | 将两个RDD中拥有相同键的数据分组到一起            | `rdd.cogroup(other)`        | {(1,([2],[])), (3, ([4, 6],[9]))}        |

用 Scala 对第二个元素进行筛选：

```scala
pairs.filter{case (key, value) => value.length < 20}
```

## 4.3.1 聚合操作

`reduceByKey()` 会为数据集中的每个键进行并行的归约操作，每个归约操作会将键相同的值合并起来。它会返回一个由各键和对应键归约出来的结果值组成的新的 RDD。

`foldByKey()` 使用一个与 RDD 和合并函数中的数据类型相同的零值作为初始值。与 `fold()`一样，`foldByKey()` 操作所使用的合并函数对零值与另一个元素进行合并，结果仍为该元素。

在 Scala 中使用 `reduceByKey()` 和 `mapValues()` 计算每个键对应的平均值：

```scala
rdd.mapValues(x => (x, 1)).reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2))”
```

![](img/chap4/img0.png)

熟悉 MapReduce 中的合并器（combiner）概念的读者可能已经注意到，调用 `reduceByKey()` 和 `foldByKey()` 会在为每个键计算全局的总结果之前先自动在每台机器上进行本地合并。用户不需要指定合并器。更泛化的 `combineByKey()` 接口可以让你自定义合并的行为。

用 Scala 实现单词计数：

```scala
val input = sc.textFile("s3://...")
val words = input.flatMap(x => x.split(" "))
val result = words.map(x => (x, 1)).reduceByKey((x, y) => x + y)”
```

`combineByKey()` 是最为常用的基于键进行聚合的函数。大多数基于键聚合的函数都是用它实现的。和 aggregate() 一样，combineByKey() 可以让用户返回与输入数据的类型不同的返回值。

`combineByKey()`会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。

如果这是一个新的元素，`combineByKey()` 会使用一个叫作 `createCombiner()` 的函数来创建那个键对应的累加器的初始值。需要注意的是，这一过程会在每个分区中第一次出现各个键时发生，而不是在整个 RDD 中第一次出现一个键时发生。

如果这是一个在处理当前分区之前已经遇到的键，它会使用 `mergeValue()`方法将该键的累加器对应的当前值与这个新的值进行合并。

由于每个分区都是独立处理的，因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器，就需要使用用户提供的 `mergeCombiners()` 方法将各个分区的结果进行合并。

`combineByKey()`有多个参数分别对应聚合操作的各个阶段，因而非常适合用来解释聚合操作各个阶段的功能划分。

`combineByKey()` 求每个键对应的平均值:

```scala
val result = input.combineByKey(
  (v) => (v, 1),
  (acc: (Int, Int), v) => (acc._1 + v, acc._2 + 1),
  (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)
  ).map{ case (key, value) => (key, value._1 / value._2.toFloat) }
  result.collectAsMap().map(println(_))
```

![](img/chap4/img1.png)

每个 RDD 都有固定数目的分区 ，分区数决定了在 RDD 上执行操作时的并行度。

在执行聚合或分组操作时，可以要求 Spark 使用给定的分区数。Spark 始终尝试根据集群的大小推断出一个有意义的默认值，但是有时候你可能要对并行度进行调优来获取更好的性能表现。

大多数操作符都能接收第二个参数，这个参数用来指定分组结果或聚合结果的RDD 的分区数，

```scala
val data = Seq(("a", 3), ("b", 4), ("a", 1))
sc.parallelize(data).reduceByKey((x, y) => x + y)    // 默认并行度
sc.parallelize(data).reduceByKey((x, y) => x + y, 10)    // 自定义并行度
```

有时，我们希望在除分组操作和聚合操作之外的操作中也能改变 RDD 的分区。对于这样的情况，Spark 提供了 `repartition()` 函数。它会把数据通过网络进行混洗，并创建出新的分区集合。切记，对数据进行重新分区是代价相对比较大的操作。Spark有个优化版的`repartition()`：`coalesce()`。

你可以使用 `rdd.partitions.size()` 查看 RDD 的分区数，并确保调用 `coalesce()` 时将 RDD 合并到比现在的分区数更少的分区中。


## 4.3.2 数据分组

如果数据已经以预期的方式提取了键，`groupByKey()` 就会使用 RDD 中的键来对数据进行分组。对于一个由类型 K 的键和类型 V 的值组成的 RDD，所得到的结果 RDD 类型会是`[K, Iterable[V]]`。

`groupBy()`可以用于未成对的数据上，也可以根据除键相同以外的条件进行分组。它可以接收一个函数，对源 RDD 中的每个元素使用该函数，将返回结果作为键再进行分组。

除了对单个 RDD 的数据进行分组，还可以使用一个叫作 `cogroup()` 的函数对多个共享同一个键的 RDD 进行分组。对两个键的类型均为 K 而值的类型分别为 V 和 W 的 RDD 进行 cogroup() 时，得到的结果 RDD 类型为 `[(K, (Iterable[V], Iterable[W]))]` 。如果其中的一个 RDD 对于另一个 RDD 中存在的某个键没有对应的记录，那么对应的迭代器则为空。`cogroup()`提供了为多个 RDD 进行数据分组的方法。

## 4.3.3 连接

连接方式多种多样：右外连接、左外连接、交叉连接以及内连接。

普通的 `join` 操作符表示内连接。只有在两个 pair RDD 中都存在的键才叫输出。当一个输入对应的某个键有多个值时，生成的 pair RDD 会包括来自两个输入 RDD 的每一组相对应的记录。

```scala
storeAddress = {
  (Store("Ritual"), "1026 Valencia St"), (Store("Philz"), "748 Van Ness Ave"),
  (Store("Philz"), "3101 24th St"), (Store("Starbucks"), "Seattle")}

storeRating = {
  (Store("Ritual"), 4.9), (Store("Philz"), 4.8))}

storeAddress.join(storeRating) == {(Store("Ritual"), ("1026 Valencia St", 4.9)),
  (Store("Philz"), ("748 Van Ness Ave", 4.8)),
  (Store("Philz"), ("3101 24th St", 4.8))}
```

`leftOuterJoin(other)` 和 `rightOuterJoin(other)` 都会根据键连接两个 RDD，但是允许结果中存在其中的一个 pair RDD 所缺失的键。

在使用 leftOuterJoin() 产生的 pair RDD 中，源 RDD 的每一个键都有对应的记录。每个键相应的值是由一个源 RDD 中的值与一个包含第二个 RDD 的值的 Option （在 Java 中为 Optional ）对象组成的二元组。

`rightOuterJoin()` 几乎与 `leftOuterJoin()` 完全一样，只不过预期结果中的键必须出现在第二个 RDD 中，而二元组中的可缺失的部分则来自于源 RDD 而非第二个 RDD。

```scala
storeAddress.leftOuterJoin(storeRating) ==
{(Store("Ritual"),("1026 Valencia St",Some(4.9))),
  (Store("Starbucks"),("Seattle",None)),
  (Store("Philz"),("748 Van Ness Ave",Some(4.8))),
  (Store("Philz"),("3101 24th St",Some(4.8)))}

storeAddress.rightOuterJoin(storeRating) ==
{(Store("Ritual"),(Some("1026 Valencia St"),4.9)),
  (Store("Philz"),(Some("748 Van Ness Ave"),4.8)),
  (Store("Philz"), (Some("3101 24th St"),4.8))}
```
