Spark使用Scala写的，运行在JVM上。

略过2.1节，下载Spark。

# 2.2 Spark中Scala的shell

Spark带有交互式的shell，可以作即时数据分析。Spark shell可以用来与分布式存储在多台机器上的的内存和硬盘上的数据进行交互，且处理过程的并发由Spark自动控制完成。

打开Scala版本的shell：

```shell
bin/spark-shell
```

控制日志的级别，要在`conf`目录下创建一个`log4j.properties`的文件：

```properties
log4j.rootCategory = WARN, console
```

在Spark中，我们通过对分布式数据集的操作来表达我们的计算意图。这些计算会自动在集群上进行。这样的数据集成为弹性分布式数据集（resilient distributed dataset，简称RDD）。RDD是Spark对分布式数据和计算的基本抽象。

```scala
scala> val lines = sc.textFile("README.md") // 创建一个名为lines的RDD
lines: spark.RDD[String] = MappedRDD[...] 

scala > lines.count() // 统计RDD中的元素个数 
res0: Long = 127 

scala > lines.first() // 这个RDD中的第一个元素，也就是README.md的第一行
res1: String = # Apache Spark
```

# 2.3 Spark核心概念简介

