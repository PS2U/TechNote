In Python, you will never really know if your program is correct until you run and test it. Not only that, unless you are able to run your program in a way that executes every possible branch of its internal control-flow, there is always some chance of a hidden error just waiting to strike.

# 11.1 Documentation Strings and the `doctest` Module

If the first line of a function, class, or module is a string, that string is known as a documentation string. The `help()` command inspects documentation strings, and Python IDEs look at the strings as well.

```python
# splitter.py
def split(line, types=None, delimiter=None):
    """Splits a line of text and optionally performs type conversion.
        For example:
        >>> split('GOOG 100 490.50')
        ['GOOG', '100', '490.50']
        >>> split('GOOG 100 490.50',[str, int, float])
        ['GOOG', 100, 490.5]
        >>>
        By default, splitting is performed on whitespace, but a different
        delimiter can be selected with the delimiter keyword argument:
        >>> split('GOOG,100,490.50',delimiter=',')
        ['GOOG', '100', '490.50']
        >>>
    """
    fields = line.split(delimiter)
    if types:
        fields = [ ty(val) for ty,val in zip(types,fields) ]
    return fields
```

A common problem with writing documentation is keeping the documentation synchronized with the actual implementation of a function. To address this problem, use the `doctest` module. `doctest` collects documentation strings, scans them for interactive sessions, and executes them as a series of tests. To use doctest, you typically create a separate module for testing. For example, if the previous function is in a file `splitter.py`, you would create a file `testsplitter.py` for testing, as follows:

```python
# testsplitter.py
import splitter
import doctest
nfail, ntests = doctest.testmod(splitter)
```

`doctest` expects the output of functions to literally match the exact output you get in the interactive interpreter.

```python
def half(x):
    """Halves x. For example:
    >>> half(6.8)
    3.4
    >>>
    """
    return x/2
```

If you run `doctest` on this function, you will get a failure report such as this:

```
**********************************************************************
File "half.py", line 4, in _ _main_ _.half
Failed example:
    half(6.8)
Expected:
    3.4
Got:
    3.3999999999999999
**********************************************************************
```


# 11.2 Unit Testing and the `unittest` Module

With unit testing, a developer writes a collection of isolated test cases for each element that makes up a program.

```python
# splitter.py
def split(line, types=None, delimiter=None):
    """Splits a line of text and optionally performs type conversion.
    ...
    """
    fields = line.split(delimiter)
    if types:
        fields = [ ty(val) for ty,val in zip(types,fields) ]
    return fields
```

```python
# testsplitter.py
import splitter
import unittest

# Unit tests
class TestSplitFunction(unittest.TestCase):
    def setUp(self):
        # Perform set up actions (if any)
        pass
    def tearDown(self):
        # Perform clean-up actions (if any)
        pass
    def testsimplestring(self):
        r = splitter.split('GOOG 100 490.50')
        self.assertEqual(r,['GOOG','100','490.50'])
    def testtypeconvert(self):
        r = splitter.split('GOOG 100 490.50',[str, int, float])
        self.assertEqual(r,['GOOG', 100, 490.5])
    def testdelimiter(self):
        r = splitter.split('GOOG,100,490.50',delimiter=',')
        self.assertEqual(r,['GOOG','100','490.50'])

# Run the unittests
if _ _name_ _ == '_ _main_ _':
    unittest.main()
```

To run tests, simply run Python on the file `testsplitter.py`. Here’s an example:

```
% python testsplitter.py
...
----------------------------------------------------------------------
Ran 3 tests in 0.014s
OK
```

Basic use of unittest involves defining a class that inherits from `unittest.TestCase`. Within this class, individual tests are defined by methods starting with the name 'test'.

An instance, `t`, of `unittest.TestCase` has the following methods that are used when writing tests and for controlling the testing process:

`t.setUp()`
Called to perform set-up steps prior to running any of the testing methods. |

`t.tearDown()`
Called to perform clean-up actions after running the tests. |

`t.assert_(expr [, msg])`
`t.failUnless(expr [, msg])`
Signals a test failure if expr evaluates as False. msg is a message string giving an explanation for the failure (if any).

`t.assertEqual(x, y [,msg])`
`t.failUnlessEqual(x, y [, msg])`
Signals a test failure if x and y are not equal to each other. msg is a message explaining the failure (if any).


# 11.3 The Python Debugger and the `pdb` Module

The `pdb` module supports post-mortem debugging, inspection of stack frames, breakpoints, single-stepping of source lines, and code evaluation.

There are several functions for invoking the debugger from a program or from the interactive Python shell.

```python
# Executes the string statement under debugger control
run(statement [, globals [, locals]])

# Evaluates the expression string under debugger control
runeval(expression [, globals [, locals]])

# Call a function within the debugger
runcall(function [, argument, ...])

# Starts the debugger at the point at which this function is called.
set_trace()

# Starts post-mortem debugging of a traceback object
post_mortem(trackback)

# Enters post-motrtem debugging using the traceback of the last exception
pm()
```

## Debugger Commands

When the debugger starts, it presents a (Pdb) prompt such as the following:

```
>>> import pdb
>>> import buggymodule
>>> pdb.run('buggymodule.start()')
> <string>(0)?()
(Pdb)
```

(Pdb) is the debugger prompt at which the following commands are recognized.

| command                           | description                              |
| --------------------------------- | ---------------------------------------- |
| [!]statement                      | Executes the (one-line) statement in the context of the current stack frame |
| a(rgs)                            | Prints the argument list of the current function |
| alias [name [command]]            | Creates an alias called name that executes command |
| b(reak) [loc [, condition]]       | Sets a breakpoint at location loc. loc either specifies a specific filename and line number |
| cl(ear) [bpnumber [bpnumber ...]] | Clears a list of breakpoint numbers      |
| commands [bpnumber]               | Sets a series of debugger commands to execute automatically when the breakpoint |
| condition bpnumber [condition]    | Places a condition on a breakpoint       |
| c(ont(inue))                      | Continues execution until the next breakpoint is encountered. |
| disable [bpnumber [bpnumber ...]] | Disables the set of specified breakpoints |
| d(own)                            | Moves the current frame one level down in the stack trace |
| h(elp) [command]                  | Shows the list of available commands     |
| ignore bpnumber [count]           | Ignores a breakpoint for count executions. |
| j(ump) lineno                     | Sets the next line to execute            |
| l(ist) [first [, last]]           | Lists source code                        |
| n(ext)                            | Executes until the next line of the current function |
| p expression                      | Evaluates the expression in the current context and prints its value |
| pp expression                     | The same as the p command, but the result is formatted using the pretty-printing module |
| q(uit)                            | Quits from the debugger                  |
| r(eturn)                          | Runs until the current function returns  |
| run [args]                        | Restarts the program and uses the command-line arguments in args as the new setting |
| s(tep)                            | Executes a single source line and stops inside called functions |
| tbreak [loc [, condition]]        | Sets a temporary breakpoint that’s removed after its first hit |
| u(p)                              | Moves the current frame one level up in the stack trace |
| unalias name                      | Deletes the specified alias              |
| until                             | Resumes execution until control leaves the current execution frame or until a line |
| w(here)                           | Prints a stack trace                     |

## Debugging from the Command Line

Alternative method for running the debugger is to invoke it on the command line:

```shell
python -m pdb someprogram.py
```

## Configuring the Debugger

If a `.pdbrc` file exists in the user’s home directory or in the current directory, it’s read in and executed as if it had been typed at the debugger prompt.


# 11.4 Program Profiling

The `profile` and `cProfile` modules are used to collect profiling information. `cProfile` is implemented as a C extension, is significantly faster, and is more modern. Either module is used to collect both coverage information (that is, what functions get executed) as well as performance statistics.

The easiest way to profile a program is to execute it from the command line as follows:

```shell
python -m cProfile someprogram.py
```

Alternatively, the following function in the profile module can be used:

```python
run(command [, filename])
```

Different parts of the report generated by run() are interpreted as follows:

| Section                   | Description                              |
| ------------------------- | ---------------------------------------- |
| primitive calls           | Number of nonrecursive function calls    |
| ncalls                    | Total number of calls (including self-recursion) |
| tottime                   | Time spent in this function (not counting subfunctions) |
| percall                   | tottime/ncalls                           |
| cumtime                   | Total time spent in the function         |
| percall                   | cumtime/(primitive calls)                |
| filename:lineno(function) | Location and name of each function       |


# 11.5 Tuning and Optimization

