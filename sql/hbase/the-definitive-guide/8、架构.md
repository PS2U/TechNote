# 8.1 数据查找和传输

B 树和 B+ 树，被广泛用于关系型存储引擎，同时还有 LSM 树（Log-Structured Merged Tree)，它在某种程度上构成了 BigTable 的底层存储架构。

# 8.1.1 B+树

B+ 树能够通过主键对记录进行高效插入、查找和删除。它表示一个动态、多层、有上下界的索引。

B+ 树能够提供高效的范围扫描功能，这得益于它的叶节点相互连接且按主键有序，避免了耗时的遍历树操作。

## 8.1.2 LSM树

LSM 树，输入数据首先被存储在日志文件，这些文件内的数据完全有序。当有日志文件被修改时，对应的更新会被先保存在内存来加速查询。

当系统经历过很多次数据修改，且内存空间被逐渐占满，LSM 树会把有序的键值对写到磁盘中，同时创建一个新的数据存储文件。此时，内存中保存的最近更新就可以丢弃了。

存储文件的组织与 B+ 树类似，不过其为磁盘顺序读取做了优化，所有节点都是满的并按页存储。修改数据文件的操作，通过滚动合并完成。也就是说，系统将现有的页与内存刷写数据混合在一起进行管理，直到数据块达到它的容量。

下图展示了多个块存储归并到磁盘的过程，这是后台线程自动完成的：

![](img/chap8/img0.png)

查询时，先查找内存中的存储，然后再查找磁盘上的文件。

删除是一种特殊的更改，当删除标记被存储后，查找会跳过这些键。当页眉重写时，有删除标记的键会被丢弃。

B 树和 LSM 树的最主要区别在于如何利用硬件，特别是磁盘。

# 8.2 存储

## 8.2.1 概览

![](img/chap8/img1.png)

从上图中可以看出，HBase 主要处理两种文件：

- 预写日志（Write-Ahead Log，WAL）
- 数据文件

两种文件主要由 HRegionServer 管理。

一个基本的流程是：

1. 客户端首先联系 ZooKeeper 子集群（quorum，一个由 ZooKeeper 节点组成的单独集群）查找行键。ZooKeeper 通过`.META`表来获取含有`-ROOT-`的 region server。
2. HBase 缓存此次查询的信息，同时直接联系实际管理数据的 `HRegionServer`。
3. `HRegionServer` 负责打开 region，并创建对应的 `HRegion` 实例。HRegion 打开后，`HRegionServer` 会为每个表的 `HColumnFamily` 创建一个 `Store` 实例。每个 `Store` 实例包含了一个或多个 `StoreFile`，它们是实际数据存储文件 `HFile` 的轻量级封装。每个 `Store` 还有个与之对应的 `MemStore`。

## 8.2.2 写路径

当用户向`HRegionServer`发起`HTable.put(Put)`请求时，会被交给对应的 `HRegion`实例处理：

1. 决定数据是否需要写到由 `HLog`类实现的 WAL 中。WAL 时标准的 Hadoop SequenceFile，并存储了`HLogKey`实例。
2. 数据被写入到 WAL 后，再被写入`MemStore`中。同时检查`MemStore`是否已满，满了就写到磁盘。刷写请求时由另外一个`HRegionServer`线程处理，它会把数据写成`HFile`。同时也会保存最后写入的序号，系统就知道哪些数据被持久化了。

## 8.2.3 文件

HBase 在 HDFS 中的目录默认为"/hbase"。`hdfs dfs -lsr /hbase`可以查看目录结构。

![](img/chap8/img2.png)

- WAL 文件。每个`HRegionServer`，都有一个对应的子目录，该子目录下有多个`HLog`文件。一个 region server的所有 region 共享同一组`HLog`文件。当日志文件包含的修改被持久化到存储文件后，他们会被移到`oldWALS`文件夹。
- `hbase.id`和`hbase.versoin`包含了集群的唯一 ID 和文件格式版本信息。
- `corrupt`文件夹把偶存了损坏的日志。
- 表级的数据都在`data`目录下。`.tmp`包括了一些临时数据。